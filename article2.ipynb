{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Учим робота готовить пиццу. Часть 2: Состязание нейронных сетей\n",
    "\n",
    "![](https://habrastorage.org/web/bf8/697/a4c/bf8697a4c0e2493e9b6fc7009bb4b83d.png)\n",
    "\n",
    "### Содержание\n",
    "* [Часть 1: Получаем данные](https://habrahabr.ru/post/335444/)\n",
    "\n",
    "В прошлой части, удалось распарсить сайт Додо-пиццы и загрузить данные об ингредиентах, а самое главное - фотографии пицц.\n",
    "Всего в нашем распоряжении оказалось 20 пицц.\n",
    "Разумеется, формировать обучающие данные всего из 20 картинок не получится. Однако, можно воспользоваться осевой симметрией пиццы:\n",
    "выполнив вращение картинки с шагом в один градус и вертикальным отражением - позволяет превратить одну фотографию в набор из 720 изображений.\n",
    "Тоже мало, но всё же попытаемся.\n",
    "\n",
    "Попробуем обучить Условный вариационный автоэнкордер (Conditional Variational Autoencoder), а потом перейдёт к тому, ради чего это всё и затевалось -  генеративным cостязательным нейронным сетям (Generative Adversarial Networks).\n",
    "<cut />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE - условный вариационный автоэнкордер\n",
    "\n",
    "Для разбирательства с автоэнкодерами, поможет отличная серия статей:\n",
    "* [Автоэнкодеры в Keras, Часть 3: Вариационные автоэнкодеры (VAE)](https://habrahabr.ru/post/331552/)\n",
    "* [Автоэнкодеры в Keras, Часть 4: Conditional VAE](https://habrahabr.ru/post/331664/)\n",
    "\n",
    "Настоятельно рекомендую к прочтению.\n",
    "Здесь же перейдём сразу к делу.\n",
    "\n",
    "Отличие CVAE от VAE - состоит в том, что нам нужно на вход как энкодеру, так и декодеру, дополнительно подавать еще метку. В нашем случае - меткой будет вектор рецепта, который получаем от OneHotEncoder.\n",
    "\n",
    "Однако, тут возникает нюанс - а в какой момент имеет смысл подавать нашу метку?\n",
    "\n",
    "Я попробовал два метода:\n",
    "1. в конце - после всех свёрток - перед полносвязным слоем\n",
    "2. в начале - после первой свёртки - добавляется как дополнительный канал\n",
    "\n",
    "В принципе, оба способа имеют право на существование. \n",
    "Кажется логичным, что если добавлять метку в конце, то она будет привязываться к более высокоуровневым фичам изображения. И наоборот - если добавлять её в начале, то она будет привязана к более низкоуровневым фичам.\n",
    "Попробуем сравнить оба способа.\n",
    "\n",
    "Вспомним, что рецепт состоит максимум из 9 ингредиентов. А их всего - 28. Получается, код рецепта будет представлять собой матрицу 9х29, а если вытянуть её, то получится 261-мерный вектор.\n",
    "\n",
    "\n",
    "Для изображения, размером 32х32, выберем размер скрытого пространства равным 512.\n",
    "Можно выбрать и меньше, но как будет видно далее - это приводит к более смазанному результату.\n",
    "\n",
    "Код для энкодера с первым методом добавления метки - после всех свёрток:\n",
    "```\n",
    "def create_conv_cvae(channels, height, width, code_h, code_w):\n",
    "\tinput_img = Input(shape=(channels, height, width))\n",
    "\t\n",
    "\tinput_code = Input(shape=(code_h, code_w))\n",
    "\tflatten_code = Flatten()(input_code)\n",
    "\t\n",
    "\tlatent_dim = 512\n",
    "\tm_height, m_width = int(height/4), int(width/4)\n",
    "\t\n",
    "\tx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "\tx = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "\tx = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "\tx = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "\tflatten_img_features = Flatten()(x)\n",
    "\tx = concatenate([flatten_img_features, flatten_code])\n",
    "\tx = Dense(1024, activation='relu')(x)\n",
    "\tz_mean = Dense(latent_dim)(x)\n",
    "\tz_log_var = Dense(latent_dim)(x)\n",
    "```\n",
    "\n",
    "Код для энкодера со вторым методом добавления метки - после первой свёртки - как дополнительный канал:\n",
    "```\n",
    "def create_conv_cvae2(channels, height, width, code_h, code_w):\n",
    "\tinput_img = Input(shape=(channels, height, width))\n",
    "\t\n",
    "\tinput_code = Input(shape=(code_h, code_w))\n",
    "\tflatten_code = Flatten()(input_code)\n",
    "\t\n",
    "\tlatent_dim = 512\n",
    "\tm_height, m_width = int(height/4), int(width/4)\n",
    "\t\n",
    "\tdef add_units_to_conv2d(conv2, units):\n",
    "\t\tdim1 = K.int_shape(conv2)[2]\n",
    "\t\tdim2 = K.int_shape(conv2)[3]\n",
    "\t\tdimc = K.int_shape(units)[1]\n",
    "\t\trepeat_n = dim1*dim2\n",
    "\t\tcount = int( dim1*dim2 / dimc)\n",
    "\t\t\n",
    "\t\tunits_repeat = RepeatVector(count+1)(units)\n",
    "\t\t#print('K.int_shape(units_repeat): ', K.int_shape(units_repeat))\n",
    "\t\tunits_repeat = Flatten()(units_repeat)\n",
    "\t\t# cut only needed lehgth of code\n",
    "\t\tunits_repeat = Lambda(lambda x: x[:,:dim1*dim2], output_shape=(dim1*dim2,))(units_repeat)\n",
    "\t\tunits_repeat = Reshape((1, dim1, dim2))(units_repeat)\n",
    "\t\treturn concatenate([conv2, units_repeat], axis=1)\n",
    "\t\n",
    "\tx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "\tx = add_units_to_conv2d(x, flatten_code)\n",
    "\t#print('K.int_shape(x): ', K.int_shape(x)) #  size here: (17, 32, 32)\n",
    "\tx = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "\tx = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "\tx = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "\tx = Flatten()(x)\n",
    "\tx = Dense(1024, activation='relu')(x)\n",
    "\tz_mean = Dense(latent_dim)(x)\n",
    "\tz_log_var = Dense(latent_dim)(x)\n",
    "```\n",
    "\n",
    "Код декодера в обоих случаях совпадает - метка добавляется в самом начале.\n",
    "```\n",
    "    z = Input(shape=(latent_dim, ))\n",
    "\tinput_code_d = Input(shape=(code_h, code_w))\n",
    "\tflatten_code_d = Flatten()(input_code_d)\n",
    "\tx = concatenate([z, flatten_code_d])\n",
    "\tx = Dense(1024)(x)\n",
    "\tx = Dense(16*m_height*m_width)(x)\n",
    "\tx = Reshape((16, m_height, m_width))(x)\n",
    "\tx = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "\tx = UpSampling2D((2, 2))(x)\n",
    "\tx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\tx = UpSampling2D((2, 2))(x)\n",
    "\tdecoded = Conv2D(channels, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "```\n",
    "\n",
    "Число параметров сети:\n",
    "1. 4'221'987\n",
    "2. 3'954'867\n",
    "\n",
    "Скорость обучения на одну эпоху:\n",
    "1. 60 сек\n",
    "2. 63 сек\n",
    "\n",
    "Результат после 40 эпох обучения:\n",
    "1. loss: -0.3232 val_loss: -0.3164\n",
    "2. loss: -0.3245  val_loss: -0.3191\n",
    "\n",
    "Как видим, второй метод требует меньше памяти для ИНС, даёт лучше результат, но требует чуть больше времени для обучения.\n",
    "\n",
    "Осталось визуально сравнить результаты работы.\n",
    "\n",
    "1. Исходное изображение (32х32)\n",
    "2. Результат работы - первый метод (latent_dim = 64)\n",
    "3. Результат работы - первый метод (latent_dim = 512)\n",
    "4. Результат работы - второй метод (latent_dim = 512)\n",
    "\n",
    "![](img/cvae/cvae_orig.png)\n",
    "![](img/cvae/cvae_decoded_latent_dim64.png)\n",
    "![](img/cvae/cvae_decoded.png)\n",
    "![](img/cvae/cvae_decoded_2.png)\n",
    "\n",
    "А теперь посмотрим, как выглядит применение переноса стиля для пиццы, когда кодирование пиццы осуществляется с оригинальным рецептом, а декодирование с другим.\n",
    "```\n",
    "i = 0\n",
    "for label in labels:\n",
    "\ti += 1\n",
    "\tlbls = []\n",
    "\tfor j in range(batch_size):\n",
    "\t\tlbls.append(label)\n",
    "\tlbls = np.array(lbls, dtype=np.float32)\n",
    "\tprint(i, lbls.shape)\n",
    "\t\n",
    "\tstt_imgs = stt.predict([orig_images, orig_labels, lbls], batch_size=batch_size)\n",
    "\tsave_images(stt_imgs, dst='temp/cvae_stt', comment='_'+str(i))\n",
    "```\n",
    "\n",
    "Результат работы переноса стиля (второй метод кодирования):\n",
    "![](img/cvae/cvae_stt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN - генеративная cостязательная сеть\n",
    "\n",
    "Мне не удалось найти устоявшегося русскоязычного названия подобных сетей.\n",
    "Варианты:\n",
    "* генеративные соревновательные сети\n",
    "* порождающие соперничающие сети\n",
    "* порождающие соревнующиеся сети\n",
    "\n",
    "Мне больше нравится:\n",
    "* генеративные cостязательные сети \n",
    "\n",
    "С теорией работы GAN опять поможет отличная серия статей:\n",
    "* [Автоэнкодеры в Keras, Часть 5: GAN(Generative Adversarial Networks) и tensorflow](https://habrahabr.ru/post/332000/)\n",
    "* [Автоэнкодеры в Keras, часть 6: VAE + GAN](https://habrahabr.ru/post/332074/)\n",
    "\n",
    "А для более глубокого понимания - свежая статья в блоге ODS: [Нейросетевая игра в имитацию](https://habrahabr.ru/company/ods/blog/322514/)\n",
    "\n",
    "Однако, начав разбираться и пробовать самостоятельно реализовать генеративную нейронную сеть - я столкнулся с некоторыми сложностями. Например, были моменты, когда генератор выдавал по-настоящему психоделические картинки.\n",
    "\n",
    "Разобраться в реализации помогли разные примеры:\n",
    "\n",
    "[MNIST Generative Adversarial Model in Keras](https://oshearesearch.com/index.php/2016/07/01/mnist-generative-adversarial-model-in-keras/) ( [mnist_gan.py](https://github.com/osh/KerasGAN/blob/master/mnist_gan.py) ),\n",
    "\n",
    "рекомендации по архитектуре из статьи конца 2015 года от [facebook research](https://research.fb.com/publications/unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks/) про **DCGAN** (Deep Convolutional GAN):\n",
    "\n",
    "[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)\n",
    "\n",
    "а так же набор рекомендаций, позволяющий заставить GAN работать:\n",
    "\n",
    "[How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks).\n",
    "\n",
    "Конструирование GAN:\n",
    "```\n",
    "def make_trainable(net, val):\n",
    "\tnet.trainable = val\n",
    "\tfor l in net.layers:\n",
    "\t\tl.trainable = val\n",
    "\n",
    "def create_gan(channels, height, width):\n",
    "\t\n",
    "\tinput_img = Input(shape=(channels, height, width))\n",
    "\t\n",
    "\tm_height, m_width = int(height/8), int(width/8)\n",
    "\t\n",
    "\t# generator\n",
    "\tz = Input(shape=(latent_dim, ))\n",
    "\tx = Dense(256*m_height*m_width)(z)\n",
    "\t#x = BatchNormalization()(x)\n",
    "\tx = Activation('relu')(x)\n",
    "\t#x = Dropout(0.3)(x)\n",
    "\t\n",
    "\tx = Reshape((256, m_height, m_width))(x)\n",
    "\n",
    "\tx = Conv2DTranspose(256, kernel_size=(5, 5), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "\t\n",
    "\tx = Conv2DTranspose(128, kernel_size=(5, 5), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "\t\n",
    "\tx = Conv2DTranspose(64, kernel_size=(5, 5), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "\t\n",
    "\tx = Conv2D(channels, (5, 5), padding='same')(x)\n",
    "\tg = Activation('tanh')(x)\n",
    "\t\n",
    "\tgenerator = Model(z, g, name='Generator')\n",
    "\t\n",
    "\t# discriminator\n",
    "\tx = Conv2D(128, (5, 5), padding='same')(input_img)\n",
    "\t#x = BatchNormalization()(x)\n",
    "\tx = LeakyReLU()(x)\n",
    "\t#x = Dropout(0.3)(x)\n",
    "\tx = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "\tx = Conv2D(256, (5, 5), padding='same')(x)\n",
    "\tx = LeakyReLU()(x)\n",
    "\tx = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "\tx = Conv2D(512, (5, 5), padding='same')(x)\n",
    "\tx = LeakyReLU()(x)\n",
    "\tx = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "\tx = Flatten()(x)\n",
    "\tx = Dense(2048)(x)\n",
    "\tx = LeakyReLU()(x)\n",
    "\tx = Dense(1)(x)\n",
    "\td = Activation('sigmoid')(x)\n",
    "\t\n",
    "\tdiscriminator = Model(input_img, d, name='Discriminator')\n",
    "\t\n",
    "\tgan = Sequential()\n",
    "\tgan.add(generator)\n",
    "\tmake_trainable(discriminator, False) #discriminator.trainable = False\n",
    "\tgan.add(discriminator)\n",
    "\t\n",
    "\treturn generator, discriminator, gan\n",
    "\n",
    "gan_gen, gan_ds, gan = create_gan(channels, height, width)\n",
    "\n",
    "gan_gen.summary()\n",
    "gan_ds.summary()\n",
    "gan.summary()\n",
    "\n",
    "opt = Adam(lr=1e-3)\n",
    "gopt = Adam(lr=1e-4)\n",
    "dopt = Adam(lr=1e-4)\n",
    "\n",
    "gan_gen.compile(loss='binary_crossentropy', optimizer=gopt)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "make_trainable(gan_ds, True)\n",
    "gan_ds.compile(loss='binary_crossentropy', optimizer=dopt)\n",
    "```\n",
    "\n",
    "Как видим, дискриминатор – это обычный бинарный классификатор, который выдаёт:\n",
    "1 - для реальных картинок,\n",
    "0 - для поддельных.\n",
    "\n",
    "Процедура обучения:\n",
    "* получаем порцию реальных картинок\n",
    "* генерируем шум, на базе которого генератор генерирует картинки\n",
    "* формируем батч для обучения дискриминатора, который состоит из реальных картинок (им присваивается метка 1) и подделок от генератора (метка 0)\n",
    "* обучаем дискриминатор\n",
    "* обучаем GAN (в нём обучается генератор, т.к. обучение дискриминатора отключено), подавая на вход шум и ожидая на выходе метку 1.\n",
    "\n",
    "```\n",
    "for epoch in range(epochs):\n",
    "\tprint('Epoch {} from {} ...'.format(epoch, epochs))\n",
    "\t\n",
    "\tn = x_train.shape[0]\n",
    "\timage_batch = x_train[np.random.randint(0, n, size=batch_size),:,:,:]    \n",
    "\t\n",
    "\tnoise_gen = np.random.uniform(-1, 1, size=[batch_size, latent_dim])\n",
    "\n",
    "\tgenerated_images = gan_gen.predict(noise_gen, batch_size=batch_size)\n",
    "\t\n",
    "\tif epoch % 10 == 0:\n",
    "\t\tprint('Save gens ...')\n",
    "\t\tsave_images(generated_images)\n",
    "\t\tgan_gen.save_weights('temp/gan_gen_weights_'+str(height)+'.h5', True)\n",
    "\t\tgan_ds.save_weights('temp/gan_ds_weights_'+str(height)+'.h5', True)\n",
    "\t\t# save loss\n",
    "\t\tdf = pd.DataFrame( {'d_loss': d_loss, 'g_loss': g_loss} )\n",
    "\t\tdf.to_csv('temp/gan_loss.csv', index=False)\n",
    "\t\n",
    "\tx_train2 = np.concatenate( (image_batch, generated_images) )\n",
    "\ty_tr2 = np.zeros( [2*batch_size, 1] )\n",
    "\ty_tr2[:batch_size] = 1\n",
    "\t\n",
    "\td_history = gan_ds.train_on_batch(x_train2, y_tr2)\n",
    "\tprint('d:', d_history)\n",
    "\td_loss.append( d_history )\n",
    "\n",
    "\tnoise_gen = np.random.uniform(-1, 1, size=[batch_size, latent_dim])\n",
    "\tg_history = gan.train_on_batch(noise_gen, np.ones([batch_size, 1]))\n",
    "\tprint('g:', g_history)\n",
    "\tg_loss.append( g_history )\n",
    "```\n",
    "\n",
    "Обратите внимание, что, в отличие от вариационного автоэнкодера, для обучение генератора не используются реальные изображения, а только метка дискриминатора. Т.е. генератор обучается на градиентах ошибки от дискриминатора.\n",
    "\n",
    "Самое интересное, что название состязательные сети - не для красивого слова - они действительно cостязаются и следить за показаниями loss-ов дискриминатора и генератора даже увлекательно.\n",
    "\n",
    "Если посмотреть на кривые потерь, то видно, что дискриминатор быстро обучается отличать реальную картинку от первоначального мусора, выдаваемого генератором, но потом кривые начинают колебаться - генератор учится генерировать всё более подходящее изображение.\n",
    "![](img/gan/gan_32_loss.png)\n",
    "\n",
    "gif-ка показывающая процесс обучения генератора (32x32) на одной пицце (первая пицца в списке - Двойная пепперони):\n",
    "![](img/gan/gan_32.gif)\n",
    "\n",
    "Как и ожидалось, результат работы GAN, по сравнению с вариационным энкодером, даёт более чёткое изображение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE + GAN - условный вариационный автоэнкордер и генеративная cостязательная сеть\n",
    "\n",
    "Осталось объединить CVAE и GAN вместе, чтобы получить лучшее от обоих сетей.\n",
    "В основе объединения лежит простая идея - декодер VAE выполняет ровно ту же функцию, что и генератор GAN, однако выполняют и  обучаются они ей по-разному.\n",
    "\n",
    "Кроме того, что не до конца понятно - как всё это заставить работать вместе, мне так же не было ясно - как в Keras-е можно применять разные функции потерь.\n",
    "Разобраться в этом вопросе помогли примеры на гитхабе:\n",
    "[Keras VAEs and GANs](https://github.com/tatsy/keras-generative)\n",
    "\n",
    "Так, применение разных функций потерь в Keras-е можно реализовать добавлением своего слоя ( [Writing your own Keras layers](https://keras.io/layers/writing-your-own-keras-layers/) ), в методе call() которого и реализовать требуемую логику рассчёта c последующим вызовом метода add_loss().\n",
    "\n",
    "Пример:\n",
    "```\n",
    "class DiscriminatorLossLayer(Layer):\n",
    "\t__name__ = 'discriminator_loss_layer'\n",
    "\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tself.is_placeholder = True\n",
    "\t\tsuper(DiscriminatorLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "\tdef lossfun(self, y_real, y_fake_f, y_fake_p):\n",
    "\t\ty_pos = K.ones_like(y_real)\n",
    "\t\ty_neg = K.zeros_like(y_real)\n",
    "\t\tloss_real = keras.metrics.binary_crossentropy(y_pos, y_real)\n",
    "\t\tloss_fake_f = keras.metrics.binary_crossentropy(y_neg, y_fake_f)\n",
    "\t\tloss_fake_p = keras.metrics.binary_crossentropy(y_neg, y_fake_p)\n",
    "\t\treturn K.mean(loss_real + loss_fake_f + loss_fake_p)\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\ty_real = inputs[0]\n",
    "\t\ty_fake_f = inputs[1]\n",
    "\t\ty_fake_p = inputs[2]\n",
    "\t\tloss = self.lossfun(y_real, y_fake_f, y_fake_p)\n",
    "\t\tself.add_loss(loss, inputs=inputs)\n",
    "\n",
    "\t\treturn y_real\n",
    "```\n",
    "\n",
    "gif-ка показывающая процесс обучения (64x64):\n",
    "![](img/cvaegan/cvaegan.gif)\n",
    "\n",
    "Результат работы переноса стиля:\n",
    "![](img/cvaegan/cvaegan_stt-.png)\n",
    "\n",
    "А теперь самое интересное! \n",
    "\n",
    "Собственно ради чего это всё и затевалось - генерация пиццы по выбранным ингредиентам.\n",
    "\n",
    "Посмотрим на пиццы с рецептом, состоящим из одного ингредиента (т.е. с кодами от 1 до 27):\n",
    "![](img/cvaegan/cvaegan_new_recipes-.png)\n",
    "\n",
    "Как и следовало ожидать - более-менее смотрятся только пиццы с самыми популярными ингредиентами 24, 20, 17 (томаты, пепперони, моцарелла) - все остальные варианты - это что-то мутное с круглой формой и непонятными серыми пятнами, в которых только при желании можно попытаться что-то угадать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "\n",
    "В целом, эксперимент можно признать частично удавшимся. Однако, даже на таком игрушечном примере чувствуется, что пафосное  выражение: \"данные - это новая нефть\" - имеет право на существование, особенно применительно к машинному обучению.\n",
    "Ведь качество работы приложения на базе машинного обучения, в первую очередь зависит от качества и количества данных.\n",
    "\n",
    "Генеративные сети - это действительно очень интересно и, думается, что в обозримом будущем мы ещё увидим множество самых различных примеров их применения.\n",
    "\n",
    "Большое спасибо за внимание!\n",
    "\n",
    "NB. При написании этой статьи - ни одна пицца не пострадала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ссылки\n",
    "\n",
    "* [Автоэнкодеры в Keras, Часть 3: Вариационные автоэнкодеры (VAE)](https://habrahabr.ru/post/331552/)\n",
    "* [Автоэнкодеры в Keras, Часть 4: Conditional VAE](https://habrahabr.ru/post/331664/)\n",
    "* [Автоэнкодеры в Keras, Часть 5: GAN(Generative Adversarial Networks) и tensorflow](https://habrahabr.ru/post/332000/)\n",
    "* [Автоэнкодеры в Keras, часть 6: VAE + GAN](https://habrahabr.ru/post/332074/)\n",
    "* Deep Convolutional GANs (DCGAN): [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)\n",
    "* [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)\n",
    "* [Keras VAEs and GANs](https://github.com/tatsy/keras-generative)\n",
    "* [MNIST Generative Adversarial Model in Keras](https://oshearesearch.com/index.php/2016/07/01/mnist-generative-adversarial-model-in-keras/) ( [mnist_gan.py](https://github.com/osh/KerasGAN/blob/master/mnist_gan.py) )\n",
    "* [Generative Adversarial Networks Part 2 - Implementation with Keras 2.0](http://www.rricard.me/machine/learning/generative/adversarial/networks/keras/tensorflow/2017/04/05/gans-part2.html)\n",
    "* [Generative Adversarial Networks with Keras](https://github.com/phreeza/keras-GAN)\n",
    "* [GAN by Example using Keras on Tensorflow Backend](https://medium.com/towards-data-science/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0)\n",
    "* [Keras implementation of Deep Convolutional Generative Adversarial Networks (DCGAN)](https://github.com/kyloon/dcgan)\n",
    "* [Генеративные модели от OpenAI](https://habrahabr.ru/company/wunderfund/blog/334568/)\n",
    "* [Нейросетевая игра в имитацию](https://habrahabr.ru/company/ods/blog/322514/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
