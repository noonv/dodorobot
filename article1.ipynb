{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Учим робота готовить пиццу. Часть 1: Получаем данные\n",
    "\n",
    "<img src=\"img/chief-robot.png\" />\n",
    "_Автор изображения: Chuchilko_\n",
    "\n",
    "Не так давно, после завершения очередного конкурса на Kaggle - вдруг возникла идея попробовать сделать тестовое ML-приложение.\n",
    "Например, такое: _\"помоги роботу сделать пиццу\"_.\n",
    "\n",
    "Разумеется, основная цель этого ровно та же - изучение нового.\n",
    "\n",
    "Захотелось разобраться, как работают генеративные нейронные сети (Generative Adversarial Networks - GAN).\n",
    "\n",
    "Ключевой идеей было обучить GAN, который по выбранным ингредиентам сам собирает картинку пиццы.\n",
    "\n",
    "Ну что ж, приступим.\n",
    "<cut />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начало\n",
    "\n",
    "Разумеется, для тренировки любого алгоритма машинного обучения, нам первым делом нужны данные.\n",
    "В нашем случае, вариантов не так много - либо находить готовый датасет, либо вытаскивать данные из интерента самотоятельно.\n",
    "\n",
    "И тут я подумал - а почему бы не дёрнуть данные с сайта Додо-пиццы.\n",
    "\n",
    "<spoiler title=\"Disclaimer\">\n",
    "Я не имею никакого отношения к данной сети пиццерий.\n",
    "Честно говоря, даже их пицца мне не особенно нравится - тем более по цене (и размерам), в моём городе (Калининград) найдутся более привлекательные пиццерии. \n",
    "</spoiler>\n",
    "\n",
    "Итак, в первом пункте плана действий появилось:\n",
    "1. получить с сайта нужные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "Так как вся нужная нам информация доступна на сайте Додо-пиццы, применим так называемый парсинг сайтов (он же - Web Scraping).\n",
    "\n",
    "Здесь нам поможет статья: [Web Scraping с помощью python](https://habrahabr.ru/post/280238/).\n",
    "\n",
    "И всего две библиотеки:\n",
    "```\n",
    "import requests\n",
    "import bs4\n",
    "```\n",
    "\n",
    "Открываем сайт додо-пиццы, щелкаем в браузере _\"Просмотреть код\"_ и находим элемент с нужными данными.\n",
    "<img src=\"img/dodopizza_scrapping_prod_pizzas.png\" />\n",
    "\n",
    "На заглавной странице можно получить только базовый список пицц и их состав. \n",
    "Более подробную информацию можно получить, кликнув на понравившийся товар. Тогда появится всплывающее окошко, с подробной информацией и красивыми картинками о пицце.\n",
    "<img src=\"img/dodopizza_scrapping_pizza_data.png\" />\n",
    "Это окошко появляется в результате GET-запроса, который можно эмулировать, передав нужные заголовки:\n",
    "```\n",
    "headers = {\n",
    "\t'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36',\n",
    "\t'Referer': siteurl,\n",
    "\t'x-requested-with': 'XMLHttpRequest'\n",
    "}\n",
    "res = requests.get(siteurl, headers = headers)\n",
    "```\n",
    "в ответ получаем кусок html-кода, который уже можно распарсить.\n",
    "\n",
    "Сразу же можно обратить внимание, что статический контент распространяется через **CDN** akamaihd.net\n",
    "\n",
    "После непродолжительных эспериментов - получился скрипт [dodo_scrapping.py](https://github.com/noonv/dodorobot/blob/master/dodo_scrapping.py) , который получает с сайта додо-пиццы название пицц, их состав, а так же сохраняет в отдельные директории по три фотографии пицц.\n",
    "\n",
    "На выходе работы скрипта получается несколько csv-файлов и директорий с фотографиями.\n",
    "Для этого выполняются следующие действия:\n",
    "- получение списка городов (на тот момент - 146)\n",
    "- получение списка пицц (для Города)\n",
    "- получение информации о пицце\n",
    "- загрузка фотографий пиццы\n",
    "\n",
    "Информацию о пицце сохраняется в табличку вида:\n",
    "город, URL города, название, названиеENG, URL пиццы, содержимое, цена, калории, углеводы, белки, жиры, диаметр, вес\n",
    "\n",
    "Что хорошо в программировании скриптов автоматизации - их можно запустить и откинувшись на списку кресла наблюдать за их работой...\n",
    "\n",
    "На выходе получилось всего 20 пицц.\n",
    "На каждую пиццу получается по 3 картинки. Нас интересует только третья картинка, на которой есть вид пиццы сверху.\n",
    "\n",
    "Разумеется, после получения картинок, их нужно дополнительно обработать - вырезать и отцентровать пиццу.\n",
    "Думаю, это не должно составить особых проблем, так как все картинки одинаковые - 710х380."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть данные с которыми мы можем работать.\n",
    "\n",
    "_Продолжение следует..._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ссылки\n",
    "\n",
    "* [Web Scraping](https://automatetheboringstuff.com/chapter11/)\n",
    "* [Web Scraping с помощью python](https://habrahabr.ru/post/280238/)\n",
    "* [dodo_scrapping.py](https://github.com/noonv/dodorobot/blob/master/dodo_scrapping.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
